# This yaml file contains configuration for running experiments with the core application
# The basic structure of the yml is:
#           task:
#             type: Train/ Inference/ Attack
#           training-options:
#             model: <Supported Model Name>
#             dataset: MNIST/CIFAR10
#             precision:              
#               bitwidth: bitW, bitA, bitG (2,2,32)
#           inference-options:
#             load: <location-of-saved-model>
#             base-model: <the model definition that the loaded model uses> 
#             images: <perform inference on saved images>
#             precision:      
#               bitwidth: bitW, bitA, bitG (2,2,32)
#           attack-options:
#             load: <location-of-saved-model>
#             base-model: <the model definition that the loaded model uses>
#             algorithm: name of the algorithm to craft adversarial examples
#             precision:
#               bitwidth: bitW, bitA, bitG (2,2,32)



task:
  #Available modes: training/ inference/ attack
  type: attack 

training-options:
  #Specify training option. Only relevant when the task type is "training"
  #Model name: (supported) LeNet5, model_a,b,c (MNIST models)
  #Model name: (supported) cifar_a, cifar_b, cifar_c (CIFAR models)
  #Model name: (supported) resnet_3, resnet_5, resnet_7 (CIFAR models)
  model: model_a
  #dataset name: (supported) mnist, cifar10  
  dataset: mnist
  epochs: 30
  precision:
  #precision to train the model on. Specify nothing to train a "Full precision model" or "bitW, bitA, bitG" for quantized model  
    bitwidth: #8,8,32  

inference-options:
  #specify inference options. Only relevant when task type is "inference"
  #load a saved model from disk. Currently only checkpoints are supported. 
  #if this is a quantized model then the precision SHOULD match the loaded model precision
  load-model: .. /logs/train_log/lenet5-mnist-FP/min-val_cross_entropy_loss

  #Give name of the model to use as a base graph. 
  #(supported) LeNet5, model_a,b,c (MNIST models)
  #(supported) cifar_a, cifar_b, cifar_c (CIFAR models)
  #(supported) resnet_3, resnet_5, resnet_7 (CIFAR models)
  base-model: model_a

  #dataset in which the model was trained on
  dataset: mnist

  #Spefify the images to perform inference on.
  #(supported): 
  #   1.  Mnist (will do inference on entire test data of mnist)
  #   2.  Cifar10 (will do inference on entire cifar test data set)
  #   3.  npz files with image and label pair. In this case give file location.
  #   4.  npz files with image, label, adversarial examples, data index. In this case give file location
  # when left blank the test split of the dataset specified above will be taken for inferencing i.e points 1 and 2 will be considered based on dataset
  images: mnist
  #precision to train the model on. Specify nothing to train a "Full precision model" or "bitW, bitA, bitG" for quantized model
  precision:
    bitwidth: #1,1,32

attack-options:
  #specify attack options. Only relevant when task type is "attack"
  #load a saved model from disk. Currently only checkpoints are supported. 
  #the app will use this model to create adversarial attacks
  
  #either create attack using supported algorithm or perform transfer attack
  attack-mode:  create #transfer/create 
  
  #enter options if you want to create attacks.
  create-attack:    
    #give a checkpoint of a model on which the adversarial images should be trained
    load-model: .. /logs/train_log/lenet5-mnist-FP/min-val_cross_entropy_loss
    #Give name of the model to use as a base graph. 
    #(supported) LeNet5, model_a,b,c (MNIST models)
    #Model name: (supported) cifar_a, cifar_b, cifar_c (CIFAR models)
    #Model name: (supported) resnet_3, resnet_5, resnet_7 (CIFAR models)
    base-model: model_a
    #Give the name of the algorithm to craft adversarial attacks on
    #(supported:)
    #    1. FGSM
    #    2. JSMA
    #    3. BA
    #    4. UAP
    #    5. cwl2   
    algorithm: FGSM
    #Set true if the samples should be selected randomly from the input dataset to create adversarial samples
    randomize: True
    #Number of samples to create
    sample-limit: 2000
    #Number of times the attack algorithm should be executed for the given sample_limit (only set >1 when randomize = true)
    iterations: 1
    #set if the benign examples should be prefiltered when creating adversarial examples. 
    #When true, only samples which are correctly classified by the classifier will be used to create adversarial examples
    prefiltered: True
    #attack parameters, a list of parameters to control attack behaviour.
    # for UAP use: [xi: (maximum allowed perturbation): value between 0 and 1, max_iter (number of iteration to run the algorithm): positive integer, eps: (perturbation per step, specific to FGSM)]
    # for FGSM use: [epsilon (attack step size): value between 0 and 1]
    # for JSMA use: [theta: (distortion introduced per selected feature per iteration): value between 0 and 1, gamma (percentage of pixels to be distorted): value between 0 and 1]
    # for cw use: [
            
            #confidence_k: Confidence of adversarial examples where a higher value produces more distorted examples but classified with higher confidence as the target class,   
            #
            #targeted: Boolean. Set 0 for false 1 for true

            #learning_rate: The initial learning rate for the attack algorithm. Smaller values produce better results but are slower to converge.

            #max_iter: The maximum number of iterations.

            #initial_const: The initial trade-off constant c to use to tune the relative importance of distance and confidence. If binary_search_steps is large, the initial constant is not important, as discussed in Carlini and Wagner (2016).

            #batch_size: Size of the batch on which adversarial samples are generated.

            #binary_search_steps: Number of times to adjust constant with binary search (positive value). If binary_search_steps is large, then the algorithm is not very sensitive to the value of initial_const. Note that the values gamma=0.999999 and c_upper=10e10 are hardcoded with the same values used by the authors of the method.

            #max_halving: Maximum number of halving steps in the line search optimization.

            #max_doubling: Maximum number of doubling steps in the line search optimization.
            #]
    # for BA use: [max_iter (number of iteration to run the algorithm)]
    parameters: [1]  
    dataset: mnist
    #precision to train the model on. Specify nothing to train a "Full precision model" or "bitW, bitA, bitG" for quantized model
    targets: [] #list of target labels. If empty then the app will assume that this is untargetted attack
    precision:
      bitwidth: #1,1,32
      
  transfer-attack:
    #provide source image in .npz format that contains adversarial images as well as corresponding labels and index
    #configuration regarding the source of adversarial images
    source:
      #source images contain adv. images created from benign images that were all classified correctly by the source classifier
      #provide the .npz file. (format:  image, label, index)
      source-images: .. /logs/trained_images/mnist-FP/mnist_conv_adv_pre-FP--run-0
      #source dataset from which the source_images were taken. This is required so that only correctly predicted images by target
      #is used during adversarial attacks
      source-data: mnist
    #location of the checkpoint containing target model parameters 
    target:
      target-model: .. /logs/train_log/lenet5-mnist-FP/min-val_cross_entropy_loss
      #taget base model. required to create graph
      target-base-model: lenet5
      #precision of target model
      precision: 
        bitwidth: #1,1,32
